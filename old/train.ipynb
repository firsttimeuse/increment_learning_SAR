{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K20c (CNMeM is enabled with initial size: 95.0% of memory, cuDNN 5110)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "'''Trains a simple convnet on the MNIST dataset.\n",
    "Does flat increment from T. Xiao  \"Error-Driven Incremental Learning in Deep Convolutional \n",
    "Neural Network for Large-Scale Image Classification\"\n",
    "Starts with just 3 classes, trains for 12 epochs then \n",
    "incrementally trains the rest of the classes by reusing \n",
    "the trained weights.\n",
    "with theano 0.9.0\n",
    "     keras 2.0.6\n",
    "'''\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1)  # for reproducibility\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D,Conv2D\n",
    "from keras.utils import np_utils\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_data(classes,total_classes,X_train_all,y_train_all,X_test_all,y_test_all):\n",
    "    '''\n",
    "    从数据库中挑选由列表classes指定的标签的数据\n",
    "    '''\n",
    "    train_ind = []\n",
    "    test_ind = []\n",
    "    for c in classes:\n",
    "        train_ind.extend(list(np.where(y_train_all==c)[0]))#找出c在y_train_all中的所有位置并按类别存入train_ind\n",
    "        test_ind.extend(list(np.where(y_test_all==c)[0]))\n",
    "\n",
    "    X_train = X_train_all[train_ind,:,:]#取出train_ind对应的训练数据\n",
    "    X_test = X_test_all[test_ind,:,:]#取出test_ind对应的测试数据\n",
    "\n",
    "    y_train_true = y_train_all[train_ind]#取出train_ind对应的训练数据标签\n",
    "    y_train = np.zeros(y_train_true.shape)#train置零\n",
    "    y_test_true = y_test_all[test_ind]#取出test_ind对应的测试数据标签\n",
    "    y_test = np.zeros(y_test_true.shape)#y_test置零\n",
    "    \n",
    "    #重设标签\n",
    "    for i,c in enumerate(classes):\n",
    "        train_ind = list(np.where(y_train_true==c)[0])\n",
    "        test_ind = list(np.where(y_test_true==c)[0])\n",
    "        y_train[train_ind] = i\n",
    "        y_test[test_ind] = i\n",
    "\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    Y_train = np_utils.to_categorical(y_train, total_classes)\n",
    "    Y_test = np_utils.to_categorical(y_test, total_classes)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_data():\n",
    "    trainX =  []\n",
    "    trainY =  []\n",
    "    testX = []\n",
    "    testY = []\n",
    "    with open('mstar_train.csv', 'r') as csv_file:\n",
    "        for data in csv.reader(csv_file):\n",
    "            # The first column is the label\n",
    "            label = int(data[0])\n",
    "            #label = np.array(label, dtype='int32')\n",
    "            trainY.append(label)\n",
    "            # The rest of columns are pixels\n",
    "            pixels = data[1:]\n",
    "\n",
    "            # Make those columns into a array of 8-bits pixels\n",
    "            # This array will be of 1D with length 784\n",
    "            # The pixel intensity values are integers from 0 to 255\n",
    "            pixels = np.array(pixels, dtype='float32')\n",
    "            pixels = pixels.reshape((128, 128))\n",
    "            trainX.append(pixels)\n",
    "    with open('mstar_test.csv', 'r') as csv_file:\n",
    "        for data in csv.reader(csv_file):\n",
    "            # The first column is the label\n",
    "            label = int(data[0])\n",
    "            #label = np.array(label, dtype='int32')\n",
    "            testY.append(label)\n",
    "            # The rest of columns are pixels\n",
    "            pixels = data[1:]\n",
    "\n",
    "            # Make those columns into a array of 8-bits pixels\n",
    "            # This array will be of 1D with length 784\n",
    "            # The pixel intensity values are integers from 0 to 255\n",
    "            pixels = np.array(pixels, dtype='float32')\n",
    "            pixels = pixels.reshape((128, 128))\n",
    "            testX.append(pixels)\n",
    "    return (np.array(trainX), np.array(trainY, dtype='int32')), (np.array(testX), np.array(testY, dtype='int32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(old_model=None):\n",
    "    model = Sequential()\n",
    "\n",
    "    if old_model is None:\n",
    "        model.add(Conv2D(nb_filters, (nb_conv, nb_conv), padding='valid',input_shape=(1, img_rows, img_cols)))\n",
    "    else:\n",
    "        weights = old_model.layers[0].get_weights()\n",
    "        model.add(Conv2D(nb_filters, (nb_conv, nb_conv), padding='valid',weights=weights, input_shape=(1, img_rows, img_cols)))\n",
    "    model.add(Activation('relu'))\n",
    "    if old_model is None:\n",
    "        model.add(Conv2D(nb_filters, (nb_conv, nb_conv)))\n",
    "    else:\n",
    "        weights = old_model.layers[2].get_weights()\n",
    "        model.add(Conv2D(nb_filters, (nb_conv, nb_conv),weights=weights))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    #model.summary()\n",
    "    if old_model is None:\n",
    "        model.add(Dense(128))\n",
    "    else:\n",
    "        weights = old_model.layers[7].get_weights()\n",
    "        model.add(Dense(128,weights=weights))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 输入图像维度\n",
    "img_rows, img_cols = 128, 128\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "#所有类的数量\n",
    "total_classes = 10\n",
    "nb_epoch = 12\n",
    "\n",
    "# 用到的卷积核数量\n",
    "nb_filters = 32\n",
    "# 卷积核的大小\n",
    "nb_conv = 3\n",
    "\n",
    "# size of pooling area for max pooling\n",
    "nb_pool = 2\n",
    "\n",
    "#拟选取的初始类别\n",
    "classes = [9,1,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "(X_train_all, y_train_all), (X_test_all, y_test_all) = load_data()\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = build_data(classes,3,\n",
    "                                              X_train_all,y_train_all,X_test_all,y_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 126, 126)      320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 126, 126)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 124, 124)      9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32, 124, 124)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 62, 62)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 62, 62)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 123008)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               15745152  \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 15,755,107\n",
      "Trainable params: 15,755,107\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = build_model()\n",
    "model1.add(Dense(len(classes)))\n",
    "model1.add(Activation('softmax'))\n",
    "\n",
    "model1.compile(loss='categorical_crossentropy',optimizer='adadelta',metrics=['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练并保存网络参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 853 samples, validate on 743 samples\n",
      "Epoch 1/12\n",
      "853/853 [==============================] - 5s - loss: 1.1012 - acc: 0.3365 - val_loss: 1.0976 - val_acc: 0.3688\n",
      "Epoch 2/12\n",
      "853/853 [==============================] - 5s - loss: 1.0983 - acc: 0.3435 - val_loss: 1.0963 - val_acc: 0.3688\n",
      "Epoch 3/12\n",
      "853/853 [==============================] - 5s - loss: 1.0983 - acc: 0.3494 - val_loss: 1.0954 - val_acc: 0.3688\n",
      "Epoch 4/12\n",
      "853/853 [==============================] - 5s - loss: 1.0981 - acc: 0.3458 - val_loss: 1.0947 - val_acc: 0.3688\n",
      "Epoch 5/12\n",
      "853/853 [==============================] - 5s - loss: 1.0973 - acc: 0.3634 - val_loss: 1.0942 - val_acc: 0.3688\n",
      "Epoch 6/12\n",
      "853/853 [==============================] - 5s - loss: 1.0968 - acc: 0.3540 - val_loss: 1.0927 - val_acc: 0.3688\n",
      "Epoch 7/12\n",
      "853/853 [==============================] - 5s - loss: 1.0957 - acc: 0.3716 - val_loss: 1.0896 - val_acc: 0.3688\n",
      "Epoch 8/12\n",
      "853/853 [==============================] - 5s - loss: 1.0909 - acc: 0.4197 - val_loss: 1.0785 - val_acc: 0.3688\n",
      "Epoch 9/12\n",
      "853/853 [==============================] - 5s - loss: 1.0654 - acc: 0.4279 - val_loss: 1.0235 - val_acc: 0.6326\n",
      "Epoch 10/12\n",
      "853/853 [==============================] - 5s - loss: 0.9910 - acc: 0.5018 - val_loss: 0.9554 - val_acc: 0.3701\n",
      "Epoch 11/12\n",
      "853/853 [==============================] - 5s - loss: 0.8749 - acc: 0.5803 - val_loss: 0.9656 - val_acc: 0.4899\n",
      "Epoch 12/12\n",
      "853/853 [==============================] - 5s - loss: 0.7959 - acc: 0.6424 - val_loss: 0.7450 - val_acc: 0.7739\n"
     ]
    }
   ],
   "source": [
    "model1.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n",
    "\n",
    "# Save this model for later interrogation\n",
    "json_string = model1.to_json() \n",
    "open('model1_incremental_architecture.json', 'w').write(json_string) \n",
    "model1.save_weights('model1_incremental_weights.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.744953671022\n",
      "Test accuracy: 0.773889636729\n"
     ]
    }
   ],
   "source": [
    "score = model1.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立新模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型并载入参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.convolutional.Conv2D at 0x7fac2d8f65d0>,\n",
       " <keras.layers.core.Activation at 0x7fac1828c050>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fac1828c1d0>,\n",
       " <keras.layers.core.Activation at 0x7fac181cbb90>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7fac181cb5d0>,\n",
       " <keras.layers.core.Dropout at 0x7fac18212d10>,\n",
       " <keras.layers.core.Flatten at 0x7fac17d64b50>,\n",
       " <keras.layers.core.Dense at 0x7fac17d0c390>,\n",
       " <keras.layers.core.Activation at 0x7fac80cc03d0>,\n",
       " <keras.layers.core.Dropout at 0x7fac17d1d350>,\n",
       " <keras.layers.core.Dense at 0x7fac2d8f6550>,\n",
       " <keras.layers.core.Activation at 0x7fac1710bed0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.Activation at 0x7fac1710bed0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 32, 126, 126)      320       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 32, 126, 126)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 124, 124)      9248      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 32, 124, 124)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 62, 62)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32, 62, 62)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 123008)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               15745152  \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 15,756,010\n",
      "Trainable params: 15,756,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#新模型在Softmax层包含了所有的10个类，用上面保存的网络权重初始化此网络，并随机初始化新类的连接\n",
    "\n",
    "# Now create a new model with all total_classes in the softmax layer.  Copy over the weights to\n",
    "# this new network and initialize the new class connections randomly.\n",
    "model2 = build_model(old_model=model1)\n",
    "model2.add(Dense(total_classes))\n",
    "\n",
    "# Replace the corresponding weights of the new network with the previously trained class weights\n",
    "weights = model2.layers[-1].get_weights()\n",
    "old_weights = model1.layers[-2].get_weights() # Last dense layer is second to last layer\n",
    "weights[0][:,-len(classes):] = old_weights[0]\n",
    "weights[1][-len(classes):] = old_weights[1]\n",
    "model2.layers[-1].set_weights(weights)\n",
    "model2.add(Activation('softmax'))\n",
    "model2.compile(loss='categorical_crossentropy',optimizer='adadelta',metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备7类数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_classes = [7, 0, 3, 5, 2, 8, 4]\n",
    "class_mapping = new_classes[:]\n",
    "class_mapping.extend(classes)\n",
    "X_train, Y_train, X_test, Y_test = build_data(new_classes, 10, X_train_all, y_train_all, X_test_all, y_test_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练新网络，保存参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1894 samples, validate on 1682 samples\n",
      "Epoch 1/12\n",
      "1894/1894 [==============================] - 13s - loss: 2.0187 - acc: 0.2149 - val_loss: 1.7540 - val_acc: 0.3876\n",
      "Epoch 2/12\n",
      "1894/1894 [==============================] - 13s - loss: 1.7460 - acc: 0.3194 - val_loss: 1.5411 - val_acc: 0.5583\n",
      "Epoch 3/12\n",
      "1894/1894 [==============================] - 13s - loss: 1.5091 - acc: 0.4192 - val_loss: 1.5749 - val_acc: 0.3609\n",
      "Epoch 4/12\n",
      "1894/1894 [==============================] - 13s - loss: 1.2804 - acc: 0.5296 - val_loss: 1.0940 - val_acc: 0.6659\n",
      "Epoch 5/12\n",
      "1894/1894 [==============================] - 13s - loss: 1.0846 - acc: 0.5993 - val_loss: 1.0875 - val_acc: 0.6118\n",
      "Epoch 6/12\n",
      "1894/1894 [==============================] - 13s - loss: 0.9327 - acc: 0.6647 - val_loss: 0.9727 - val_acc: 0.6445\n",
      "Epoch 7/12\n",
      "1894/1894 [==============================] - 13s - loss: 0.7908 - acc: 0.7307 - val_loss: 0.6470 - val_acc: 0.8347\n",
      "Epoch 8/12\n",
      "1894/1894 [==============================] - 13s - loss: 0.6678 - acc: 0.7756 - val_loss: 0.5100 - val_acc: 0.8787\n",
      "Epoch 9/12\n",
      "1894/1894 [==============================] - 13s - loss: 0.5768 - acc: 0.8163 - val_loss: 0.5033 - val_acc: 0.8234\n",
      "Epoch 10/12\n",
      "1894/1894 [==============================] - 13s - loss: 0.4886 - acc: 0.8379 - val_loss: 0.4104 - val_acc: 0.8912\n",
      "Epoch 11/12\n",
      "1894/1894 [==============================] - 13s - loss: 0.4195 - acc: 0.8754 - val_loss: 0.4495 - val_acc: 0.8668\n",
      "Epoch 12/12\n",
      "1894/1894 [==============================] - 13s - loss: 0.3946 - acc: 0.8833 - val_loss: 0.3747 - val_acc: 0.8954\n"
     ]
    }
   ],
   "source": [
    "model2.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n",
    "\n",
    "# Save the incrementally trained model\n",
    "json_string = model2.to_json() \n",
    "open('model2_incremental_architecture.json', 'w').write(json_string) \n",
    "model2.save_weights('model2_incremental_weights.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估新网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.374709201602\n",
      "Test accuracy: 0.895362663283\n"
     ]
    }
   ],
   "source": [
    "score = model2.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试新网络的泛化性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400/2425 [============================>.] - ETA: 0s\n",
      "Total Test score: 4.04302053612\n",
      "Total Test accuracy: 0.621030927835\n"
     ]
    }
   ],
   "source": [
    "X_test = X_test_all.reshape(X_test_all.shape[0], 1, img_rows, img_cols)\n",
    "X_test = X_test.astype('float32')\n",
    "X_test /= 255\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "# Note, that when a new image is presented to this network, the label of the image must be \n",
    "# fed into class_mapping to get the \"real\" label of the output\n",
    "y_test = np.array([class_mapping.index(c) for c in y_test_all])\n",
    "Y_test = np_utils.to_categorical(y_test, total_classes)\n",
    "\n",
    "score = model2.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "print('\\nTotal Test score:', score[0])\n",
    "print('Total Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "272px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
